# GrabAR: Occlusion-aware Grabbing Virtual Objects in AR

<p align="middle">
    <img src="res/loupe.gif" width="200"><img src="res/radio.gif" width="200"><img src="res/phone.gif" width="200">
</p>


##  Introduction

This repo is an official **[PyTorch](https://pytorch.org/)** implementation of [**GrabAR: Occlusion-aware Grabbing Virtual Objects in AR**](https://wbstx.github.io/grabar/).

## How to use

1. Download the [hand segmetation network weights](https://drive.google.com/file/d/1LVsDIP3pYUHDmSsZobtPgQ1pDrNbVLwq/view?usp=sharing) and the [occlusion estimation network weights](https://drive.google.com/file/d/1IHJJaGDoG7cgpZWO-81gOfjxY0Si2tDE/view?usp=sharing), put them in the weights folder.
2. Simply run **GrabAR.py**, you will see results in the **Image** folder

## Dataset

https://drive.google.com/drive/folders/1GnEysk5-zo7ao8fAbiqNR_fyy_L_BL-P?usp=sharing

## Links

[Project](https://wbstx.github.io/grabar/) [Paper](https://arxiv.org/abs/1912.10637) [Video](https://youtu.be/zixCh8GzTzE)

