# GrabAR: Occlusion-aware Grabbing Virtual Objects in AR

<p align="middle">
    <img src="res/loupe.gif" width="200"><img src="res/radio.gif" width="200"><img src="res/phone.gif" width="200">
</p>


##  Introduction

This repo is an official **[PyTorch](https://pytorch.org/)** implementation of [**GrabAR: Occlusion-aware Grabbing Virtual Objects in AR**](https://wbstx.github.io/grabar/).

## How to use

1. Download the [hand segmetation network weights](https://drive.google.com/file/d/1LVsDIP3pYUHDmSsZobtPgQ1pDrNbVLwq/view?usp=sharing) and the [occlusion estimation network weights](https://drive.google.com/file/d/1IHJJaGDoG7cgpZWO-81gOfjxY0Si2tDE/view?usp=sharing), put them in the weights folder.
2. Simply run **GrabAR.py**, you will see results in the **Image** folder

## Dataset

Coming soon...

## Links

[Project](https://wbstx.github.io/grabar/) [Paper](https://arxiv.org/abs/1912.10637) [Video](https://youtu.be/zixCh8GzTzE)

